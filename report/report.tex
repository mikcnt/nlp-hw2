%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{graphicx}

\renewcommand{\UrlFont}{\ttfamily\small}
\graphicspath{ {./images/} }

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{float}


\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Homework 2: Aspect-Based Sentiment Analysis}

\author{Michele Conti \\
	\texttt{conti.1599133@studenti.uniroma1.it}\\}

\date{}

\begin{document}
	\maketitle
	\section{Introduction}
	Aspect-based sentiment analysis (ABSA) tackles the problem of finding both aspects in a given sentence, and the polarities associated with those aspects. For example, given the sentence "I love their pasta but I hate their Ananas Pizza", the task is to first extract the two aspects "pasta" and "Ananas Pizza", and to then to predict their associated polarities, in this case respectively "positive" and "negative".
	
	In this paper, I propose an end-to-end approach to attack this task, exploiting the combination of both context-independent (GloVe) and context-dependent (BERT) word embeddings, multihead attention layers, and the use of a BiLSTM layer, reaching satisfying results on the provided development dataset (macro f1 score of 0.544 on the Restaurants + Laptops development set).
	
	\section{Problem formulation}
	All the model variations proposed in this paper tackle the ABSA problem in an end-to-end fashion, meaning that both tasks (i.e., aspect sentiment extraction and aspect sentiment polarity evaluation) are solved simultaneously. To achieve this, the two-sided problem has been formulated as a sequence labeling task. 
	
	To that end, this is the general pipeline followed: during the preprocessing, each token of each sentence is tagged using either an IOB (i.e., Inside, Outside, Beginning) or a BIOES (i.e., Beginning, Inside, Outside, Ending, Single) tagging schema, used on top of the four polarities appearing in the datasets (example of the two tagging schemas in figure \ref{fig:IOB_BIOES}). Depending on the adopted tagging schema, the model is then trained to extract the right label from a set of either  $4 \cdot 2 + 1 = 9$ (for IOB) or $4 \cdot 4 + 1 = 17$ (for BIOES) possible classes. Finally, the model output is translated back into its original form, where the tokens classified as "O" are not considered and the aspect terms are merged together according to their tags (e.g., \{"Ananas" = B-negative, "Pizza" = I-negative\} $\to$ \{"Ananas Pizza" = negative\}).
	
	\section{Models}
	In this section, I will present all the modules that compose the proposed final model, starting by introducing the structure of the baseline network. Then, at each step, I will introduce the very next module used, describing its main functionalities and analyzing (possibly) the boost in terms of performance when added to the network.
	
	To evaluate the performance of the following models, I will use their macro f1 score on the combined task of aspect term extraction and evaluation.
	
	Moreover, IOB has been selected as the preferred tagging schema, and all the following model performances are computed using that. Several experiments have been conducted to find out the best one, but, due to space reasons, only the comparisons for the proposed model are reported here.
	
	\subsection{M0: Baseline}
	The first model used to solve the ABSA task is a very simple network, composed by an embedding layer to encode the tokens of the input sentences, followed by a one-on-one linear layer (i.e., input features equal to output features), two stacked BiLSTM layers and a prediction block, composed by a linear layer and a softmax activation (network architecture in figure \ref{fig:M0_architecture}). Additionally, I replaced the randomly initialized word embeddings in the embedding layer with the pre-trained word vectors of GloVe \citep{pennington2014glove}.
	
	In this first model variation, no contextual information is considered in the embedding phase, since the word embeddings provided by GloVe, being context-free, will produce the same vector representation regardless of the context. Nevertheless, the BiLSTM layers partially make up for this, encoding both the left-to-right and the right-to-left context for each word of a sentence.
	
	This very simple architecture (let's call it M0) is still able to achieve decent results in terms of f1 score, reaching a maximum of 0.381 in terms of macro f1 score (loss plots, extraction f1 score, extraction + evaluation f1 score respectively on figures \ref{fig:M0_loss}, \ref{fig:M0_extr}, \ref{fig:M0_eval}).
	
	\subsection{M1: BERT Embedder}
	To add contextual information to my model, I decided to use BERT (Bidirectional Encoder Representations Transformers) \citep{devlin2018bert}. BERT is a language representation model, which uses bidirectional transformers to pre-train on a very large corpus over different pre-training tasks. Then, the pre-trained BERT model can be fine-tuned to be used on other tasks. In my case, I decided to use BERT as an embeding layer, as suggested in \citep{li2019exploiting}. Differently from the traditional GloVe based embedding layer, which, as I already said, provide only context-independent representation for each token, using BERT as an embedding layer does provide context-dependant information about each token in a sentence.
	
	To get the most out of BERT, I followed one of the approaches proposed in \citep{acs2021subword} as pooling strategy. In particular, the approach can be summarized in two steps: first, the BERT embeddings for the subwords of each token are computed. This is achieved by averaging the last 3 hidden layers of BERT. Then, the weighted average of the subwords embeddings are computed, based on the BPE length of each token.
	
	This process can be carried out in two ways: keeping the BERT parameters freezed, or keeping them free and finetuning them during the training process. These two possible choices are the basis of a trade off between two possible scenarios: on the one hand, keeping the BERT weights freezed, we have a much easier time when training our model, in terms of both training time, and flexibility in the choice of the hyperparameters. On the other, learning also the parameters for the BERT model gives the chance to, potentially, achieve much better results.
	
	Once the BERT embeddings have been computed, similarly to the GloVe embeddings, they are too passed into a linear layer with a ReLU activation, and subsequently concatenated with the GloVe pretrained embeddings. The rest of the network is the same. Adding context-dependent information with BERT and keeping its parameters freezed (let's call this model M1), the model reaches 0.466 macro f1 score on the dev set.
	
	\subsection{M2: Finetuning BERT}
	In order to finetune the BERT embedder layer, I had to tweak some of the hyperparameters I was using before. For instance, once the parameters of this embedding layers are free to be updated, the chosen learning rate really starts to make a difference. Decreasing it by an order of magnitude seemed to do the trick. This way, I was able to achieve much better results, reaching 0.497 macro f1 score on the development set.
	
	\subsection{M3: Self-Attention}
	Self-attention is an attention mechanism relating different positions
	of a single sequence in order to compute a representation of the sequence. It has been made popular by \citep{vaswani2017attention}, and, intuitively, it makes it possible for the network to focus on relevant parts of a sentence. This is achieved in the following way:
	
	\begin{equation}
		\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
	\end{equation}
	
	To attend different parts of a sentence, multiple attention heads can be computed and then concatenated. This concatenation can be then linearly transformed into the expected dimension. This process is known as Multi-head Attention.
	
	For my use case, I used Multi-head Attention on both the GloVe embeddings and on the BERT embeddings, respectively using 12 and 24 heads. Then, as in the previous versions, I concatenated the two resulting embeddings, and passed the concatenation to the stacked BiLSTM layers.
	
	This was the trick that made me achieve the best results, achieving 0.544 macro f1 score on the development set.
	
	\clearpage
	\section{Figures and tables}
	All figures and tables have been intentionally placed at the end of the document, in this section, so that they don't affect the maximum limit of three pages for the text.
	
	\begin{table}[H]
		\centering
		\begin{tabular}{@{}lcc@{}}
			\toprule
			\textbf{Sentiment} & Train & Dev \\ \midrule
			positive           & 2605  & 546 \\
			negative           & 1364  & 307 \\
			neutral            & 877   & 216 \\
			conflict           & 111   & 25  \\ \bottomrule
		\end{tabular}
		\caption{Polarities support for targets (task A+B) in the training and development set (i.e., Restaurants + Laptops).}
		\label{tab:my-table}
	\end{table}
	
	\begin{table}[H]
		\centering
		\begin{tabular}{@{}lcc@{}}
			\toprule
			\textbf{Category}         & Train     & Dev \\ \midrule
			anecdotes/miscellaneous   & 941       & 191     \\
			price                     & 268       & 53      \\
			food                      & 1008      & 224     \\
			ambience                  & 355       & 76      \\
			service                   & 478       & 119     \\ \bottomrule
		\end{tabular}
		\caption{Categories support (task C+D) in the training and development set (i.e., Restaurants).}
		\label{tab:my-table}
	\end{table}
	
	\begin{table}[H]
		\centering
		\begin{tabular}{@{}lcc@{}}
			\toprule
			\textbf{Sentiment} & Train & Dev \\ \midrule
			positive           & 1803  & 376 \\
			negative           & 672   & 167 \\
			neutral            & 411   & 89  \\
			conflict           & 164   & 31  \\ \bottomrule
		\end{tabular}
		\caption{Polarities support for categories (task C+D) in the training and development set (i.e., Restaurants).}
		\label{tab:my-table}
	\end{table}
	
	
	\begin{table}[H]
		\centering
		\begin{tabular}{@{}lcc@{}}
			\toprule
			\textbf{Model}         & F1 Extr.       & F1 Eval.       \\ \midrule
			M0 = Baseline          & 0.715          & 0.381          \\
			M1 = M0 + BERT$_{freeze}$         & 0.805          & 0.466          \\
			M2 = M0 + BERT$_{finetune}$        & 0.801          & 0.497          \\
			\textbf{M3 = M2 + ATT} & \textbf{0.820} & \textbf{0.544} \\
			M4 = M3 + CRF          & 0.813          & 0.493          \\
			M5 = M3 + POS          & 0.798          & 0.509          \\ \bottomrule
		\end{tabular}
		\caption{Task A+B: Models variations top performance on development set (Restaurants + Laptops). For each variation, it has been selected the model with the highest f1 evaluation score on the validation set. \\ The baseline model (M0) is the one discussed in section (?). With "BERT" I refer to the BERT Embedder introduced in section (?) (BERT$_{freeze}$ means that the BERT Embedder is based on a pretrained version of the BERT model, and that there is no finetuning involved during the training. Instead, BERT$_{finetune}$ means that the BERT model is initially loaded from a pretrained version, but it is also finetuned during training). With "ATT" I refer to a multihead self attention layer, as discussed in section (?). With "CRF" I refer to Conditional Random Field, as discussed in section (?). With "POS", I refer to the use of an embedding layer for the part-of-speech tags of the tokens of each sentence, as discussed in section (?).}
		\label{tab:my-table}
	\end{table}

	\begin{table}[H]
		\centering
		\begin{tabular}{@{}lcc@{}}
			\toprule
			\textbf{Model}                        & F1 Extr.       & F1 Eval.       \\ \midrule
			\textbf{M3$_{\mathbf{IOB}}$} & \textbf{0.820} & \textbf{0.544} \\
			M3$_{\mathrm{BIOES}}$                         & 0.812          & 0.490          \\ \bottomrule
		\end{tabular}
		\caption{Task A+B: Proposed model top performance comparison using two tagging schemas: IOB (i.e., Inside-Outside-Beginning), and BIOES (i.e., beginning, inside, outside, end, single).}
		\label{tab:my-table}
	\end{table}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{IOB_BIOES_example.pdf}
		\caption{IOB and BIOES tagging schemas example.}
		\label{fig:IOB_BIOES}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\columnwidth]{M0_diagram.pdf}
		\caption{Baseline model (M0) architecture.}
		\label{fig:M0_architecture}
	\end{figure}
	
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\columnwidth]{M3_diagram.pdf}
		\caption{Final model (M3) architecture.}
		\label{fig:M3_architecture}
	\end{figure}
	
	

	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{M0_ab_loss.png}
		\caption{Task A+B: Baseline (M0) loss on the training and development set.}
		\label{fig:M0_loss}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{M3_ab_loss.png}
		\caption{Task A+B: Final model (M3) loss on the training and development set.}
		\label{fig:M3_loss}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{M0_ab_f1_extr.png}
		\caption{Task A+B: Baseline (M0) macro f1 extraction score on the training and development set.}
		\label{fig:M0_extr}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{M3_ab_f1_extr.png}
		\caption{Task A+B: Final model (M3) macro f1 extraction score on the training and development set.}
		\label{fig:M3_extr}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{M0_ab_f1_eval.png}
		\caption{Task A+B: Baseline (M0) macro f1 evaluation score on the training and development set.}
		\label{fig:M0_eval}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{M3_ab_f1_eval.png}
		\caption{Task A+B: Final model (M3) macro f1 evaluation score on the training and development set.}
		\label{fig:M3_eval}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{ab_comparative_f1_extr.png}
		\caption{Task A+B: Comparative plot of f1 extraction score for the 6 model variations (M0, M1, M2, M3, M4, M5).}
		\label{fig:comparative_f1_extr}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{ab_comparative_f1_eval.png}
		\caption{Task A+B: Comparative plot of f1 evaluation score for the 6 model variations (M0, M1, M2, M3, M4, M5).}
		\label{fig:comparative_f1_eval}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{M3_ab_IOB_vs_BIOES_f1_extr.png}
		\caption{Task A+B: Comparative plot of f1 extraction score for the best model (M3) using two different tagging schemas (IOB, BIOES).}
		\label{fig:IOB_vs_BIOES_extr}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{M3_ab_IOB_vs_BIOES_f1_eval.png}
		\caption{Task A+B: Comparative plot of f1 evaluation score for the best model (M3) using two different tagging schemas (IOB, BIOES).}
		\label{fig:IOB_vs_BIOES_eval}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{M0_ab_confusion_matrix.png}
		\caption{Task A+B: Normalized confusion matrix of the baseline model (M0).}
		\label{fig:cm_M0}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=1\columnwidth]{M3_ab_confusion_matrix.png}
		\caption{Task A+B: Normalized confusion matrix of the best model (M3).}
		\label{fig:cm_M0}
	\end{figure}
	
	\bibliographystyle{acl_natbib}
	\bibliography{bibliography}
	
	%\appendix
	
	
	
\end{document}